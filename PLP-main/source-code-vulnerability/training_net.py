# -*- coding: utf-8 -*-
"""
Created on Wed Apr 21 22:08:30 2021

@author: arash
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from modules import Net
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
#from torchvision import transforms
from data_loader import my_collate, CustomIterableDataset, CustomDatasetBalanced
import random, pickle
import numpy as np
import sys
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP


def training_network(dataset):
    #N=17481
    N = 20086
    batch_size = 32
    if torch.cuda.is_available():
        device = torch.device("cuda:0")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
    else:
        device = torch.device("cpu")

    if torch.cuda.is_available():
        model = Net(input_dim=N, batch_size=batch_size).cuda()
    else:
        model = Net(input_dim=N, batch_size=batch_size).to(device)
    model = nn.DataParallel(model, device_ids=[0], output_device=[0])
    model.to(device)
    #print('model')

    #loss_function = nn.MSELoss()
    if torch.cuda.is_available():
        loss_function = nn.BCEWithLogitsLoss().cuda()
    else:
        loss_function = nn.BCEWithLogitsLoss().to(device)
    #loss_function = nn.BCELoss().cuda()
    
    #optimizer = optim.SGD(model.parameters(), lr=0.01, momentum =0.99)
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    loss_tot = []
    temp_loss = 0
    for epoch in range(10):
        if epoch % 1 == 0:
            print('\nEpoch ', epoch, '---> Loss ', temp_loss)

        dataset.randdata()
        X = DataLoader(dataset, batch_size=batch_size, collate_fn=my_collate, num_workers=10, shuffle=True)

        temp_loss = 0
        iterations = 0
        for local_batch, local_labels in X:
            sys.stdout.write('\r')
            sys.stdout.write(f'PROGRESS: {round((iterations * 100) / X.__len__(), 2)} %')
            local_batch=torch.permute(local_batch, (1, 0, 2))
            #print(local_batch.size())
            #print(local_labels.size())
            optimizer.zero_grad()
            if torch.cuda.is_available():
                tar = model(local_batch.float().cuda())#, torch.Tensor(local_labels).float().cuda())
            else:
                tar = model(local_batch.float().to(device))
            #print(tar[:,0])
            #print(local_labels)
            #print(local_labels.size(), tar[:,0].size())
            if torch.cuda.is_available():
                loss = loss_function(tar[:,0].cuda(), local_labels.float().cuda())
            else:
                loss = loss_function(tar[:, 0].to(device), local_labels.float().to(device))
            temp_loss += loss.detach().item()
            loss.backward()
            optimizer.step()
            iterations += 1
         
        name = 'model3N' + str(epoch) + '.pt'
        torch.save(model, name)    
        loss_tot.append(temp_loss)   
        pickle.dump(loss_tot, open('loss3N.p', 'wb'))

    return model, loss_tot      


if __name__ == '__main__':
    training_network(CustomDatasetBalanced())

















